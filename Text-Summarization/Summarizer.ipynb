{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import collections\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "WORD = re.compile(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps = \"([A-Z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"An army soldier was injured in fierce gun battle with a group of infiltrating terrorists from across the Line of Control in Gali Maidan area of Sawjian sector, while a BSF jawan was injured in unprovoked firing by Pakistani rangers in Hiranagar sector along the international border in Kathua district on Friday.Identifying the injured army soldier as Launce Naik Vinod Kumar and the BSF jawan as Gurnam Singh, sources said that former got injured during an encounter with a group of terrorists who sneaked into Sawjian sector on the Indian side from the Pakistan occupied Kashmir during wee hours of Friday. The encounter was in progress, sources added.Significantly, the infiltration attempt from across the LoC in Sawjian sector of Poonch district came nearly 24 hours after half a dozen heavily armed terrorists attacked a BSF naka along the international border in Kathua district with small arms fire and rocket propelled grenades so as to cross over to the Indian side. The infiltration attempt was foiled by alert BSF personnel who retaliated killing one of them as during illumation of the area with the help of tracer bomb, terrorists fleeing back to Pakistan side were seen carrying a body with them, sources added.Meanwhile, a BSF jawan was injured as Pakistani Rangers continued resorting to mortar shelling and small arms fire on two forward Indian positions at Bobiyan in Hiranagar sector of Kathua district. Sources said that the fire from across the international border first came around 9.35 am and it continued for nearly 40 minutes.Thereafter, the Pakistani Rangers again resumed firing on Indian side around 12.15 noon, sources said, adding that it was continuing till reports last came in. The Indian side was also retaliating.Ever since, India carried out surgical strikes across the Line of Control causing sufficient damage to terrorists and those shielding them last month, Pakistan has been resorting to mortar shelling and small arms fire at one or the other place along the borders in Jammu region. It continued lobbing mortar shells, besides resorting to automatics and small arms fire along the LoC in Rajouri district’s Manjakote area of Bhimber Gali sector throughout Wednesday night.The Indian troops retaliated appropriately. There had been no casualty or damage on the Indian side. Pakistani troops have resorted to firing in Rajouri sector also this afternoon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = split_into_sentences(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(sentences) :\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences :\n",
    "        tokens = []\n",
    "        split = sentence.lower().split()\n",
    "        for word in split :\n",
    "            if word not in stop :\n",
    "                try :\n",
    "                    stemmer.stem(word)\n",
    "                    tokens.append(word)\n",
    "                except :\n",
    "                    tokens.append(word)\n",
    "        \n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = remove_stop_words(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posTagger(tokenized_sentences) :\n",
    "    tagged = []\n",
    "    for sentence in tokenized_sentences :\n",
    "        tag = nltk.pos_tag(sentence)\n",
    "        tagged.append(tag)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged = posTagger(remove_stop_words(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfIsf(tokenized_sentences):\n",
    "    scores = []\n",
    "    COUNTS = []\n",
    "    for sentence in tokenized_sentences :\n",
    "        counts = collections.Counter(sentence)\n",
    "        isf = []\n",
    "        score = 0\n",
    "        for word in counts.keys() :\n",
    "            count_word = 1\n",
    "            for sen in tokenized_sentences :\n",
    "                for w in sen :\n",
    "                    if word == w :\n",
    "                        count_word += 1\n",
    "            score = score + counts[word]*math.log(count_word-1)\n",
    "        scores.append(score/len(sentence))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfIsfScore = tfIsf(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similar(tokens_a, tokens_b) :\n",
    "    #Using Jaccard similarity to calculate if two sentences are similar\n",
    "    ratio = len(set(tokens_a).intersection(tokens_b))/ float(len(set(tokens_a).union(tokens_b)))\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarityScores(tokenized_sentences) :\n",
    "    scores = []\n",
    "    for sentence in tokenized_sentences :\n",
    "        score = 0;\n",
    "        for sen in tokenized_sentences :\n",
    "            if sen != sentence :\n",
    "                score += similar(sentence,sen)\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similarityScore = similarityScores(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def properNounScores(tagged) :\n",
    "    scores = []\n",
    "    for i in range(len(tagged)) :\n",
    "        score = 0\n",
    "        for j in range(len(tagged[i])) :\n",
    "            if(tagged[i][j][1]== 'NNP' or tagged[i][j][1]=='NNPS') :\n",
    "                score += 1\n",
    "        scores.append(score/float(len(tagged[i])))\n",
    "    return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "properNounScore = properNounScores(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def centroidSimilarity(sentences) :\n",
    "    centroidIndex = tfIsfScore.index(max(tfIsfScore))\n",
    "    scores = []\n",
    "    for sentence in sentences :\n",
    "        vec1 = text_to_vector(sentences[centroidIndex])\n",
    "        vec2 = text_to_vector(sentence)\n",
    "        \n",
    "        score = get_cosine(vec1,vec2)\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return collections.Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroidSimilarityScore = centroidSimilarity(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numericToken(tokenized_sentences):\n",
    "    scores = []\n",
    "    for sentence in tokenized_sentences :\n",
    "        score = 0\n",
    "        for word in sentence :\n",
    "            if is_number(word) :\n",
    "                score +=1 \n",
    "        scores.append(score/float(len(sentence)))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numericTokenScore = numericToken(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02857142857142857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.4,\n",
       " 0.1111111111111111,\n",
       " 0.09090909090909091,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericTokenScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RBM:\n",
    "  \n",
    "    def __init__(self, num_visible, num_hidden, learning_rate = 0.1):\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_visible = num_visible\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize a weight matrix, of dimensions (num_visible x num_hidden), using\n",
    "        # a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
    "        self.weights = 0.1 * np.random.randn(self.num_visible, self.num_hidden)    \n",
    "        # Insert weights for the bias units into the first row and first column.\n",
    "        self.weights = np.insert(self.weights, 0, 0, axis = 0)\n",
    "        self.weights = np.insert(self.weights, 0, 0, axis = 1)\n",
    "\n",
    "    def train(self, data, max_epochs = 1000):\n",
    "        \"\"\"\n",
    "        Train the machine.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: A matrix where each row is a training example consisting of the states of visible units.    \n",
    "        \"\"\"\n",
    "\n",
    "        num_examples = data.shape[0]\n",
    "\n",
    "        # Insert bias units of 1 into the first column.\n",
    "        data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "        for epoch in range(max_epochs):      \n",
    "            # Clamp to the data and sample from the hidden units. \n",
    "            # (This is the \"positive CD phase\", aka the reality phase.)\n",
    "            pos_hidden_activations = np.dot(data, self.weights)      \n",
    "            pos_hidden_probs = self._logistic(pos_hidden_activations)\n",
    "            pos_hidden_states = pos_hidden_probs > np.random.rand(num_examples, self.num_hidden + 1)\n",
    "            # Note that we're using the activation *probabilities* of the hidden states, not the hidden states       \n",
    "            # themselves, when computing associations. We could also use the states; see section 3 of Hinton's \n",
    "            # \"A Practical Guide to Training Restricted Boltzmann Machines\" for more.\n",
    "            pos_associations = np.dot(data.T, pos_hidden_probs)\n",
    "\n",
    "            # Reconstruct the visible units and sample again from the hidden units.\n",
    "            # (This is the \"negative CD phase\", aka the daydreaming phase.)\n",
    "            neg_visible_activations = np.dot(pos_hidden_states, self.weights.T)\n",
    "            neg_visible_probs = self._logistic(neg_visible_activations)\n",
    "            neg_visible_probs[:,0] = 1 # Fix the bias unit.\n",
    "            neg_hidden_activations = np.dot(neg_visible_probs, self.weights)\n",
    "            neg_hidden_probs = self._logistic(neg_hidden_activations)\n",
    "            # Note, again, that we're using the activation *probabilities* when computing associations, not the states \n",
    "            # themselves.\n",
    "            neg_associations = np.dot(neg_visible_probs.T, neg_hidden_probs)\n",
    "\n",
    "            # Update weights.\n",
    "            self.weights += self.learning_rate * ((pos_associations - neg_associations) / num_examples)\n",
    "\n",
    "            error = np.sum((data - neg_visible_probs) ** 2)\n",
    "            print(\"Epoch %s: error is %s\" % (epoch, error))\n",
    "\n",
    "    def run_visible(self, data):\n",
    "        \"\"\"\n",
    "        Assuming the RBM has been trained (so that weights for the network have been learned),\n",
    "        run the network on a set of visible units, to get a sample of the hidden units.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: A matrix where each row consists of the states of the visible units.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        hidden_states: A matrix where each row consists of the hidden units activated from the visible\n",
    "        units in the data matrix passed in.\n",
    "        \"\"\"\n",
    "    \n",
    "        num_examples = data.shape[0]\n",
    "    \n",
    "        # Create a matrix, where each row is to be the hidden units (plus a bias unit)\n",
    "        # sampled from a training example.\n",
    "        hidden_states = np.ones((num_examples, self.num_hidden + 1))\n",
    "    \n",
    "        # Insert bias units of 1 into the first column of data.\n",
    "        data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "        # Calculate the activations of the hidden units.\n",
    "        hidden_activations = np.dot(data, self.weights)\n",
    "        # Calculate the probabilities of turning the hidden units on.\n",
    "        hidden_probs = self._logistic(hidden_activations)\n",
    "        # Turn the hidden units on with their specified probabilities.\n",
    "        hidden_states[:,:] = hidden_probs > np.random.rand(num_examples, self.num_hidden + 1)\n",
    "        # Always fix the bias unit to 1.\n",
    "        # hidden_states[:,0] = 1\n",
    "  \n",
    "        # Ignore the bias units.\n",
    "        hidden_states = hidden_states[:,1:]\n",
    "        return hidden_states\n",
    "    \n",
    "      # TODO: Remove the code duplication between this method and `run_visible`?\n",
    "    def run_hidden(self, data):\n",
    "        \"\"\"\n",
    "        Assuming the RBM has been trained (so that weights for the network have been learned),\n",
    "        run the network on a set of hidden units, to get a sample of the visible units.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: A matrix where each row consists of the states of the hidden units.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        visible_states: A matrix where each row consists of the visible units activated from the hidden\n",
    "        units in the data matrix passed in.\n",
    "        \"\"\"\n",
    "\n",
    "        num_examples = data.shape[0]\n",
    "\n",
    "        # Create a matrix, where each row is to be the visible units (plus a bias unit)\n",
    "        # sampled from a training example.\n",
    "        visible_states = np.ones((num_examples, self.num_visible + 1))\n",
    "\n",
    "        # Insert bias units of 1 into the first column of data.\n",
    "        data = np.insert(data, 0, 1, axis = 1)\n",
    "\n",
    "        # Calculate the activations of the visible units.\n",
    "        visible_activations = np.dot(data, self.weights.T)\n",
    "        # Calculate the probabilities of turning the visible units on.\n",
    "        visible_probs = self._logistic(visible_activations)\n",
    "        # Turn the visible units on with their specified probabilities.\n",
    "        visible_states[:,:] = visible_probs > np.random.rand(num_examples, self.num_visible + 1)\n",
    "        # Always fix the bias unit to 1.\n",
    "        # visible_states[:,0] = 1\n",
    "\n",
    "        # Ignore the bias units.\n",
    "        visible_states = visible_states[:,1:]\n",
    "        return visible_states\n",
    "    \n",
    "    def daydream(self, num_samples):\n",
    "        \"\"\"\n",
    "        Randomly initialize the visible units once, and start running alternating Gibbs sampling steps\n",
    "        (where each step consists of updating all the hidden units, and then updating all of the visible units),\n",
    "        taking a sample of the visible units at each step.\n",
    "        Note that we only initialize the network *once*, so these samples are correlated.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        samples: A matrix, where each row is a sample of the visible units produced while the network was\n",
    "        daydreaming.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a matrix, where each row is to be a sample of of the visible units \n",
    "        # (with an extra bias unit), initialized to all ones.\n",
    "        samples = np.ones((num_samples, self.num_visible + 1))\n",
    "\n",
    "        # Take the first sample from a uniform distribution.\n",
    "        samples[0,1:] = np.random.rand(self.num_visible)\n",
    "\n",
    "        # Start the alternating Gibbs sampling.\n",
    "        # Note that we keep the hidden units binary states, but leave the\n",
    "        # visible units as real probabilities. See section 3 of Hinton's\n",
    "        # \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
    "        # for more on why.\n",
    "        for i in range(1, num_samples):\n",
    "            visible = samples[i-1,:]\n",
    "\n",
    "            # Calculate the activations of the hidden units.\n",
    "            hidden_activations = np.dot(visible, self.weights)      \n",
    "            # Calculate the probabilities of turning the hidden units on.\n",
    "            hidden_probs = self._logistic(hidden_activations)\n",
    "            # Turn the hidden units on with their specified probabilities.\n",
    "            hidden_states = hidden_probs > np.random.rand(self.num_hidden + 1)\n",
    "            # Always fix the bias unit to 1.\n",
    "            hidden_states[0] = 1\n",
    "\n",
    "            # Recalculate the probabilities that the visible units are on.\n",
    "            visible_activations = np.dot(hidden_states, self.weights.T)\n",
    "            visible_probs = self._logistic(visible_activations)\n",
    "            visible_states = visible_probs > np.random.rand(self.num_visible + 1)\n",
    "            samples[i,:] = visible_states\n",
    "\n",
    "        # Ignore the bias units (the first column), since they're always set to 1.\n",
    "        return samples[:,1:]        \n",
    "      \n",
    "    def _logistic(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureMatrix = []\n",
    "featureMatrix.append(tfIsfScore)\n",
    "featureMatrix.append(similarityScore)\n",
    "featureMatrix.append(properNounScore)\n",
    "featureMatrix.append(centroidSimilarityScore)\n",
    "featureMatrix.append(numericTokenScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureMat = np.zeros((len(sentences),5))\n",
    "for i in range(5) :\n",
    "    for j in range(len(sentences)):\n",
    "        featureMat[j][i] = featureMatrix[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.86940882,  1.02818169,  0.03125   ,  0.08703883,  0.        ],\n",
       "       [ 0.68771034,  0.90492397,  0.        ,  0.09759001,  0.        ],\n",
       "       [ 0.74893307,  0.28492572,  0.        ,  0.3086067 ,  0.        ],\n",
       "       [ 0.74172371,  1.0611716 ,  0.02857143,  0.0955637 ,  0.02857143],\n",
       "       [ 0.47510274,  0.56542171,  0.        ,  0.16666667,  0.        ],\n",
       "       [ 0.94103793,  1.14057058,  0.        ,  0.14664712,  0.        ],\n",
       "       [ 0.92873014,  0.64550781,  0.        ,  0.        ,  0.1       ],\n",
       "       [ 0.35835189,  0.10323887,  0.        ,  0.        ,  0.4       ],\n",
       "       [ 0.84543007,  0.83376428,  0.        ,  0.23570226,  0.11111111],\n",
       "       [ 0.30919976,  0.27863146,  0.        ,  0.10910895,  0.09090909],\n",
       "       [ 1.00633792,  0.72368287,  0.        ,  1.        ,  0.        ],\n",
       "       [ 0.55926804,  0.66769524,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.63790778,  0.70765032,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.83305113,  0.59300105,  0.        ,  0.36514837,  0.        ],\n",
       "       [ 0.83305113,  0.5261562 ,  0.        ,  0.24618298,  0.        ],\n",
       "       [ 0.79451346,  0.59275926,  0.        ,  0.11785113,  0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: error is 12.5705350434\n",
      "Epoch 1: error is 11.8924939428\n",
      "Epoch 2: error is 10.8092534158\n",
      "Epoch 3: error is 10.6251062488\n",
      "Epoch 4: error is 9.59061449731\n",
      "Epoch 5: error is 8.82908432089\n",
      "Epoch 6: error is 8.53310418976\n",
      "Epoch 7: error is 8.29510186952\n",
      "Epoch 8: error is 7.96225971308\n",
      "Epoch 9: error is 8.01786390034\n",
      "Epoch 10: error is 6.84320566896\n",
      "Epoch 11: error is 8.10648836096\n",
      "Epoch 12: error is 6.90113398027\n",
      "Epoch 13: error is 7.29009056922\n",
      "Epoch 14: error is 6.39279281296\n",
      "Epoch 15: error is 6.4011764619\n",
      "Epoch 16: error is 6.4586746505\n",
      "Epoch 17: error is 5.14061269873\n",
      "Epoch 18: error is 4.93790049839\n",
      "Epoch 19: error is 4.8948400015\n",
      "Epoch 20: error is 5.01264392535\n",
      "Epoch 21: error is 4.6528021184\n",
      "Epoch 22: error is 5.22160019041\n",
      "Epoch 23: error is 5.30392794684\n",
      "Epoch 24: error is 5.21730424855\n",
      "Epoch 25: error is 4.59349879049\n",
      "Epoch 26: error is 4.56882921776\n",
      "Epoch 27: error is 4.37703589139\n",
      "Epoch 28: error is 4.20268104582\n",
      "Epoch 29: error is 3.82272526414\n",
      "Epoch 30: error is 5.07627181693\n",
      "Epoch 31: error is 4.09928767453\n",
      "Epoch 32: error is 4.12722963209\n",
      "Epoch 33: error is 3.63207459747\n",
      "Epoch 34: error is 3.23843571442\n",
      "Epoch 35: error is 3.62862177899\n",
      "Epoch 36: error is 3.93185093686\n",
      "Epoch 37: error is 3.87783175814\n",
      "Epoch 38: error is 4.1263938213\n",
      "Epoch 39: error is 3.98953855956\n",
      "Epoch 40: error is 3.40126024697\n",
      "Epoch 41: error is 3.90542973785\n",
      "Epoch 42: error is 3.4898317374\n",
      "Epoch 43: error is 3.73639654803\n",
      "Epoch 44: error is 3.83239437824\n",
      "Epoch 45: error is 4.61185591441\n",
      "Epoch 46: error is 3.3174280893\n",
      "Epoch 47: error is 3.74851245398\n",
      "Epoch 48: error is 3.36890179396\n",
      "Epoch 49: error is 3.69816231636\n",
      "Epoch 50: error is 3.5766905511\n",
      "Epoch 51: error is 3.76867825159\n",
      "Epoch 52: error is 3.61078767213\n",
      "Epoch 53: error is 3.48938063314\n",
      "Epoch 54: error is 3.01468386053\n",
      "Epoch 55: error is 3.82208634944\n",
      "Epoch 56: error is 2.99304242257\n",
      "Epoch 57: error is 3.09894083538\n",
      "Epoch 58: error is 3.20869621193\n",
      "Epoch 59: error is 3.41480028029\n",
      "Epoch 60: error is 3.48895218246\n",
      "Epoch 61: error is 3.21554277751\n",
      "Epoch 62: error is 3.47171309601\n",
      "Epoch 63: error is 4.18626001402\n",
      "Epoch 64: error is 2.98210041197\n",
      "Epoch 65: error is 3.6876831371\n",
      "Epoch 66: error is 3.07961368987\n",
      "Epoch 67: error is 2.77626717945\n",
      "Epoch 68: error is 3.5652540556\n",
      "Epoch 69: error is 3.72722752914\n",
      "Epoch 70: error is 4.02062602301\n",
      "Epoch 71: error is 3.40147522189\n",
      "Epoch 72: error is 3.49673254215\n",
      "Epoch 73: error is 2.88760724563\n",
      "Epoch 74: error is 3.66201367653\n",
      "Epoch 75: error is 3.02477658354\n",
      "Epoch 76: error is 3.57722238337\n",
      "Epoch 77: error is 3.32265049243\n",
      "Epoch 78: error is 2.93121518733\n",
      "Epoch 79: error is 3.52968537132\n",
      "Epoch 80: error is 3.44202832855\n",
      "Epoch 81: error is 4.44498191445\n",
      "Epoch 82: error is 2.79760774399\n",
      "Epoch 83: error is 3.43539588739\n",
      "Epoch 84: error is 3.5145165635\n",
      "Epoch 85: error is 3.30459533884\n",
      "Epoch 86: error is 3.39587096129\n",
      "Epoch 87: error is 3.37378830778\n",
      "Epoch 88: error is 3.16651129073\n",
      "Epoch 89: error is 3.02542036327\n",
      "Epoch 90: error is 2.81227443245\n",
      "Epoch 91: error is 3.48297688731\n",
      "Epoch 92: error is 3.71953801898\n",
      "Epoch 93: error is 3.27828097359\n",
      "Epoch 94: error is 3.42725367389\n",
      "Epoch 95: error is 3.16459400088\n",
      "Epoch 96: error is 3.79724610657\n",
      "Epoch 97: error is 3.36713419213\n",
      "Epoch 98: error is 3.18415133065\n",
      "Epoch 99: error is 2.75891775919\n",
      "[[ 0.27951944  0.23497819  0.32603184  0.34785428  0.28612507  0.24297774]\n",
      " [ 0.27919074  0.29230116  0.49082425  0.358079    0.15449334  0.14715917]\n",
      " [ 0.19453765  0.17780516  0.39315442  0.3429492   0.12272693  0.05486975]\n",
      " [-0.82613783 -0.81004729 -0.90377834 -0.89322667 -0.87126481 -0.67115989]\n",
      " [-0.34684833 -0.42192375 -0.56734326 -0.49327211 -0.17348853 -0.27874257]\n",
      " [-0.69550941 -0.53138001 -0.78183846 -0.83065397 -0.70432155 -0.7188558 ]]\n",
      "[[ 1.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "r = RBM(num_visible = 5, num_hidden = 5)\n",
    "#training_data = np.array([[0.5,0.9,0.7,0,0.8],[0.3,0,1,0.4,0],[0.8,1,1,0,0.6],[0,0.5,1,1,1], [0,0,1,1,0],[0,0,1,1,0.6]])\n",
    "training_data = featureMat\n",
    "r.train(training_data, max_epochs = 100)\n",
    "print(r.weights)\n",
    "user = np.array([[0,0.4,0,0.5,1.2]])\n",
    "print(r.run_visible(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8694088201505211,\n",
       "  0.6877103442418614,\n",
       "  0.7489330683884977,\n",
       "  0.741723711545762,\n",
       "  0.47510274314232553,\n",
       "  0.9410379332760858,\n",
       "  0.928730141311231,\n",
       "  0.358351893845611,\n",
       "  0.8454300699101399,\n",
       "  0.30919976196928683,\n",
       "  1.0063379226837872,\n",
       "  0.5592680420245436,\n",
       "  0.6379077800624592,\n",
       "  0.833051127543801,\n",
       "  0.833051127543801,\n",
       "  0.7945134575869864],\n",
       " [1.0281816913591968,\n",
       "  0.9049239690280328,\n",
       "  0.28492572242572245,\n",
       "  1.0611715988870607,\n",
       "  0.565421705676495,\n",
       "  1.140570583864726,\n",
       "  0.645507813739113,\n",
       "  0.10323886639676114,\n",
       "  0.8337642837642838,\n",
       "  0.2786314562630352,\n",
       "  0.7236828711828712,\n",
       "  0.6676952438755844,\n",
       "  0.7076503235620234,\n",
       "  0.593001053001053,\n",
       "  0.526156204906205,\n",
       "  0.5927592622714574],\n",
       " [0.03125,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.02857142857142857,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.08703882797784893,\n",
       "  0.09759000729485333,\n",
       "  0.3086066999241838,\n",
       "  0.09556369651349933,\n",
       "  0.16666666666666666,\n",
       "  0.1466471150213533,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.23570226039551587,\n",
       "  0.1091089451179962,\n",
       "  1.0000000000000002,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.3651483716701107,\n",
       "  0.2461829819586655,\n",
       "  0.11785113019775793],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.02857142857142857,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.4,\n",
       "  0.1111111111111111,\n",
       "  0.09090909090909091,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
